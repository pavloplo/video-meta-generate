# Database
# Local: use a local Postgres instance.
# Staging/Prod: use the managed database connection string.
# Local Docker example:
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/app?schema=public"
DIRECT_URL="postgresql://postgres:postgres@localhost:5432/app?schema=public"

# Sessions
# Local: readable cookie name for local development.
# Staging/Prod: use a secure cookie name (e.g. prefix with __Secure-).
SESSION_COOKIE_NAME="video-meta-generate-session"
SESSION_TTL_DAYS="30"

# ============================================
# File Storage Configuration (Optional)
# ============================================
# Storage is required for file uploads (POST /api/upload)
# Configure either Supabase Storage (development) or AWS S3 (production)

# --- Option 1: Supabase Storage (Development) ---
# Get these from your Supabase project settings > API
# STORAGE_ENDPOINT=https://<project-id>.supabase.co/storage/v1/s3
# STORAGE_REGION=us-east-1
# STORAGE_BUCKET=uploads
# STORAGE_ACCESS_KEY_ID=<your-supabase-access-key>
# STORAGE_SECRET_ACCESS_KEY=<your-supabase-secret-key>
# STORAGE_FORCE_PATH_STYLE=true

# --- Option 2: AWS S3 (Production) ---
# Get these from AWS IAM > Access Keys
# STORAGE_ENDPOINT=https://s3.us-east-1.amazonaws.com
# STORAGE_REGION=us-east-1
# STORAGE_BUCKET=your-bucket-name
# STORAGE_ACCESS_KEY_ID=<your-aws-access-key>
# STORAGE_SECRET_ACCESS_KEY=<your-aws-secret-key>
# STORAGE_FORCE_PATH_STYLE=false

# Storage Setup Instructions:
# 
# For Supabase Storage:
# 1. Create a Supabase project at https://supabase.com
# 2. Go to Settings > API
# 3. Create a storage bucket named "uploads" (or your preferred name)
# 4. Set bucket to public if you want direct URL access
# 5. Get your project ID from the URL: https://<project-id>.supabase.co
# 6. For access keys, you may need to use the service role key or create S3-compatible credentials
#    (Supabase Storage supports S3-compatible API)
#
# For AWS S3:
# 1. Create an S3 bucket in your AWS account
# 2. Create an IAM user with S3 read/write permissions
# 3. Generate access keys for the IAM user
# 4. Set the bucket name and region
# 5. For production, consider using IAM roles instead of access keys
#
# Note: Storage configuration is optional. If not set, file uploads will fail with a 503 error.
# The upload endpoint will work once storage is properly configured.

# --- Option 3: Local MinIO (Docker) ---
# Use this for local development with Docker Compose
# Start MinIO: docker compose --profile local up -d minio
# Access MinIO Console: http://localhost:9001 (login: minioadmin/minioadmin)
# Create a bucket named "uploads" in the MinIO console
STORAGE_ENDPOINT=http://localhost:9000
STORAGE_REGION=us-east-1
STORAGE_BUCKET=uploads
STORAGE_ACCESS_KEY_ID=minioadmin
STORAGE_SECRET_ACCESS_KEY=minioadmin
STORAGE_FORCE_PATH_STYLE=true
